version: '3.5'

services:
  api:
    container_name: protollm-api
    image: llm-api-image
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - ${API_PORT}:6672
    env_file:
      - .env
    volumes:
      - ./unit_config.json:/docker-entrypoint.d/unit_config.json
    networks:
      - llm_wrap_network

  rabbitmq:
    image: "rabbitmq:3-management"
    ports:
      - ${RABBIT_MQ_PORT}:5672
      - ${WEB_RABBIT_MQ}:15672
    env_file:
      - .env
    volumes:
      - rabbitmq_data:/var/lib/rabbitmq
    networks:
      - llm_wrap_network

  redis:
    image: "redis:alpine"
    ports:
      - ${REDIS_PORT}:6379
    volumes:
      - redis_data:/var/lib/data
    networks:
      - llm_wrap_network

  llm:
    container_name: protollm-worker
    image: llm-core:latest
    #    runtime: nvidia
    #    deploy:
    #      resources:
    #        limits:
    #          # cpus: 5
    #          memory: 100G
    build:
      context: .
      dockerfile: Dockerfile
    env_file: .env
    #    volumes:
    #      - <your_path_to_data_in_docker>:/data
    ports:
      - ${LLM_WORKER_PORT}:8672
    networks:
      - llm_wrap_network
    restart: unless-stopped
    environment:
      - PYTHONPATH=/app
    command: ["python", "protollm_api/worker/main.py"]

networks:
  llm_wrap_network:
    name: llm_wrap_network
    driver: bridge

volumes:
  rabbitmq_data:
  redis_data:
